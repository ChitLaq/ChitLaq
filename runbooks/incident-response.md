# ChitLaq M1 MVP - Incident Response Runbook

> **Generated by PROMPT 1.7** - Infrastructure Documentation & Knowledge Transfer  
> **Senior Technical Writer & Infrastructure Architect** - 15+ years system documentation and knowledge transfer experience

## Executive Summary

This incident response runbook provides comprehensive procedures for handling various types of incidents in the ChitLaq M1 MVP infrastructure. It serves as a step-by-step guide for operations team members to effectively detect, respond to, and resolve incidents while minimizing impact on users and maintaining system reliability.

## Table of Contents
- [Incident Classification](#incident-classification)
- [Response Team Structure](#response-team-structure)
- [Communication Procedures](#communication-procedures)
- [Detection and Alerting](#detection-and-alerting)
- [Response Procedures](#response-procedures)
- [Recovery Procedures](#recovery-procedures)
- [Post-Incident Review](#post-incident-review)
- [Common Incident Scenarios](#common-incident-scenarios)
- [Escalation Procedures](#escalation-procedures)
- [Tools and Resources](#tools-and-resources)

## Incident Classification

### Severity Levels

#### Critical (P0) - Service Down
- **Definition**: Complete service outage, data loss, security breach
- **Impact**: All users affected, core functionality unavailable
- **Response Time**: Immediate (within 15 minutes)
- **Resolution Target**: 1 hour
- **Examples**:
  - Database corruption or loss
  - Complete application failure
  - Security breach or data exposure
  - Infrastructure failure (VPS down)

#### High (P1) - Service Degraded
- **Definition**: Significant service degradation, partial outage
- **Impact**: Many users affected, core functionality impaired
- **Response Time**: 1 hour
- **Resolution Target**: 4 hours
- **Examples**:
  - API response times > 5 seconds
  - Database performance issues
  - WebSocket connection failures
  - Authentication service issues

#### Medium (P2) - Minor Issues
- **Definition**: Minor service issues, non-critical bugs
- **Impact**: Some users affected, non-core functionality impaired
- **Response Time**: 4 hours
- **Resolution Target**: 24 hours
- **Examples**:
  - Search functionality issues
  - Feed loading delays
  - Minor UI bugs
  - Performance degradation

#### Low (P3) - Non-Critical
- **Definition**: Feature requests, minor bugs, documentation issues
- **Impact**: Minimal user impact, non-essential functionality
- **Response Time**: 24 hours
- **Resolution Target**: 1 week
- **Examples**:
  - Feature requests
  - Minor UI improvements
  - Documentation updates
  - Non-critical bug fixes

### Incident Categories

#### Infrastructure Incidents
- **VPS/Server Issues**: Hardware failure, network issues, power outages
- **Docker/Container Issues**: Container failures, resource exhaustion
- **Database Issues**: Connection problems, performance issues, corruption
- **Network Issues**: DNS problems, connectivity issues, firewall issues

#### Application Incidents
- **API Issues**: Endpoint failures, response time issues, error rates
- **WebSocket Issues**: Connection failures, message delivery issues
- **Authentication Issues**: Login failures, session management issues
- **Performance Issues**: Slow response times, high resource usage

#### Security Incidents
- **Authentication Breaches**: Unauthorized access, credential compromise
- **DDoS Attacks**: Traffic spikes, resource exhaustion
- **Malware/Intrusion**: System compromise, data breach
- **SSL/TLS Issues**: Certificate problems, encryption failures

#### Data Incidents
- **Data Loss**: Database corruption, backup failures
- **Data Corruption**: Inconsistent data, integrity issues
- **Backup Issues**: Backup failures, restoration problems
- **Migration Issues**: Data migration failures, schema changes

## Response Team Structure

### Incident Commander
- **Role**: Overall incident coordination and decision making
- **Responsibilities**:
  - Coordinate response efforts
  - Make critical decisions
  - Communicate with stakeholders
  - Escalate when necessary
- **Contact**: +1-XXX-XXX-XXXX

### Technical Lead
- **Role**: Technical investigation and resolution
- **Responsibilities**:
  - Investigate root cause
  - Implement fixes
  - Coordinate technical team
  - Document technical details
- **Contact**: +1-XXX-XXX-XXXX

### Communications Lead
- **Role**: External and internal communication
- **Responsibilities**:
  - Update status page
  - Communicate with users
  - Coordinate with PR team
  - Manage social media updates
- **Contact**: +1-XXX-XXX-XXXX

### Operations Team
- **Role**: Infrastructure and system management
- **Responsibilities**:
  - Monitor systems
  - Execute recovery procedures
  - Maintain infrastructure
  - Support technical team
- **Contact**: +1-XXX-XXX-XXXX

## Communication Procedures

### Internal Communication

#### Incident Channel Setup
```bash
# Create incident Slack channel
# Channel naming: #incident-YYYY-MM-DD-HHMM-severity
# Example: #incident-2024-01-01-1200-p0

# Channel template:
# ðŸš¨ INCIDENT: [Title]
# Severity: P0/P1/P2/P3
# Status: Investigating/Identified/Monitoring/Resolved
# Impact: [Description]
# ETA: [Time]
# Commander: [Name]
# Technical Lead: [Name]
```

#### Status Updates
- **Frequency**: Every 15 minutes for P0, every 30 minutes for P1, every hour for P2/P3
- **Format**: Standardized status update template
- **Channels**: Incident channel, team channels, management channels

#### Escalation Communication
- **P0**: Immediate phone call to incident commander
- **P1**: Slack message + email to team lead
- **P2**: Slack message to team
- **P3**: Ticket creation in issue tracker

### External Communication

#### Status Page Updates
```bash
# Update status page
curl -X POST https://status.chitlaq.com/api/incidents \
  -H "Content-Type: application/json" \
  -d '{
    "title": "Service Outage",
    "status": "investigating",
    "impact": "major",
    "description": "We are currently investigating a service outage affecting our platform.",
    "eta": "2024-01-01T13:00:00Z"
  }'
```

#### Social Media Updates
- **Twitter**: @ChitLaqStatus
- **Format**: Standardized incident update template
- **Frequency**: Major updates only

#### User Communication
- **Email**: Automated notifications to affected users
- **In-app**: Banner notifications for active users
- **Support**: Updated FAQ and support responses

## Detection and Alerting

### Alert Sources

#### Prometheus Alerts
```bash
# Check active alerts
curl -s http://localhost:9093/api/v1/alerts | jq '.data[] | select(.status.state == "active")'

# Check alert rules
curl -s http://localhost:9090/api/v1/rules | jq '.data.groups[].rules[]'

# Check alert history
curl -s http://localhost:9093/api/v1/alerts | jq '.data[] | select(.status.state == "resolved")' | tail -10
```

#### System Monitoring
```bash
# Check system health
./scripts/health-check.sh

# Check service status
docker compose ps

# Check resource usage
htop
free -h
df -h
```

#### Application Monitoring
```bash
# Check API health
curl -s http://localhost:3001/health | jq .

# Check database health
docker compose exec postgres pg_isready -U postgres

# Check Redis health
docker compose exec redis redis-cli ping
```

### Alert Response

#### Immediate Response
1. **Acknowledge Alert**: Confirm receipt and start investigation
2. **Assess Impact**: Determine severity and user impact
3. **Classify Incident**: Assign severity level and category
4. **Activate Team**: Notify appropriate team members
5. **Start Communication**: Begin status updates

#### Investigation Steps
1. **Gather Information**: Collect logs, metrics, and system state
2. **Identify Symptoms**: Document observed issues
3. **Determine Scope**: Assess affected systems and users
4. **Hypothesize Cause**: Form initial theories about root cause
5. **Test Hypotheses**: Validate or refute theories

## Response Procedures

### Initial Response (0-15 minutes)

#### P0/P1 Incidents
```bash
# 1. Acknowledge incident
echo "ðŸš¨ INCIDENT ACKNOWLEDGED: $(date)" >> /var/log/incidents.log

# 2. Check system status
./scripts/health-check.sh > /tmp/health-check-$(date +%Y%m%d-%H%M%S).log

# 3. Gather initial information
docker compose ps > /tmp/docker-status-$(date +%Y%m%d-%H%M%S).log
docker compose logs --tail=100 > /tmp/docker-logs-$(date +%Y%m%d-%H%M%S).log

# 4. Check active alerts
curl -s http://localhost:9093/api/v1/alerts | jq '.data[] | select(.status.state == "active")' > /tmp/active-alerts-$(date +%Y%m%d-%H%M%S).json

# 5. Notify team
# Send Slack message to incident channel
# Call incident commander if P0
```

#### P2/P3 Incidents
```bash
# 1. Create incident ticket
# Use issue tracker to create incident ticket

# 2. Gather basic information
./scripts/health-check.sh

# 3. Check relevant logs
docker compose logs SERVICE_NAME --tail=50

# 4. Notify team via Slack
```

### Investigation Phase (15-60 minutes)

#### System Investigation
```bash
# Check system resources
htop
iostat 1 5
free -h
df -h

# Check network connectivity
ping -c 3 google.com
ping -c 3 your-domain.com
telnet your-domain.com 443

# Check service dependencies
docker compose ps
docker stats --no-stream
```

#### Application Investigation
```bash
# Check application logs
docker compose logs --tail=200

# Check API endpoints
curl -s http://localhost:3001/health | jq .
curl -s http://localhost:3002/health | jq .
curl -s http://localhost:3003/health | jq .
curl -s http://localhost:3004/health | jq .
curl -s http://localhost:3005/health | jq .

# Check database
docker compose exec postgres psql -U postgres -c "SELECT 1;"
docker compose exec postgres psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"

# Check Redis
docker compose exec redis redis-cli ping
docker compose exec redis redis-cli info memory
```

#### Performance Investigation
```bash
# Check response times
curl -s "http://localhost:9090/api/v1/query?query=histogram_quantile(0.95,rate(http_request_duration_seconds_bucket[5m]))" | jq .

# Check error rates
curl -s "http://localhost:9090/api/v1/query?query=rate(http_requests_total{status=~'5..'}[5m])" | jq .

# Check database performance
curl -s "http://localhost:9090/api/v1/query?query=postgresql_query_duration_seconds" | jq .
```

### Resolution Phase (1-4 hours)

#### Quick Fixes
```bash
# Restart failed services
docker compose restart SERVICE_NAME

# Clear caches
docker compose exec redis redis-cli FLUSHALL

# Restart Nginx
docker compose restart nginx

# Restart monitoring
docker compose restart prometheus grafana
```

#### Configuration Changes
```bash
# Update configuration
# Edit relevant config files
# Restart services
docker compose restart

# Verify changes
./scripts/health-check.sh
```

#### Code Deployments
```bash
# Deploy hotfix
git checkout hotfix/incident-fix
docker compose build
docker compose up -d

# Verify deployment
./scripts/health-check.sh
curl -s http://localhost:3001/health | jq .
```

## Recovery Procedures

### Service Recovery

#### Database Recovery
```bash
# Check database status
docker compose exec postgres pg_isready -U postgres

# Check database connections
docker compose exec postgres psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"

# Restart database if needed
docker compose restart postgres

# Verify database recovery
docker compose exec postgres psql -U postgres -c "SELECT 1;"
```

#### Application Recovery
```bash
# Check application status
docker compose ps

# Restart applications
docker compose restart

# Verify application recovery
curl -s http://localhost:3001/health | jq .
curl -s http://localhost:3002/health | jq .
curl -s http://localhost:3003/health | jq .
curl -s http://localhost:3004/health | jq .
curl -s http://localhost:3005/health | jq .
```

#### Infrastructure Recovery
```bash
# Check VPS status
uptime
free -h
df -h

# Check Docker daemon
sudo systemctl status docker

# Restart Docker if needed
sudo systemctl restart docker

# Restart all services
docker compose up -d
```

### Data Recovery

#### Database Backup Restoration
```bash
# List available backups
ls -la /opt/backups/chitlaq/

# Restore from backup
./backup-scripts/restore.sh /opt/backups/chitlaq/backup_YYYYMMDD_HHMMSS.sql

# Verify restoration
docker compose exec postgres psql -U postgres -c "SELECT count(*) FROM users;"
```

#### Configuration Recovery
```bash
# Restore configuration
tar -xzf /opt/backups/chitlaq/config_latest.tar.gz -C /

# Restart services
docker compose restart

# Verify configuration
./scripts/health-check.sh
```

### Performance Recovery

#### Resource Optimization
```bash
# Check resource usage
htop
iostat 1 5
free -h

# Optimize memory usage
docker system prune -f

# Optimize disk usage
find /var/lib/docker -type f -size +100M -exec ls -lh {} \;
```

#### Cache Recovery
```bash
# Clear application caches
docker compose exec redis redis-cli FLUSHALL

# Restart cache services
docker compose restart redis

# Verify cache recovery
docker compose exec redis redis-cli ping
```

## Post-Incident Review

### Incident Documentation

#### Incident Report Template
```markdown
# Incident Report: [Incident ID]

## Summary
- **Date**: YYYY-MM-DD
- **Time**: HH:MM - HH:MM (UTC)
- **Duration**: X hours Y minutes
- **Severity**: P0/P1/P2/P3
- **Impact**: [Description]

## Timeline
- **Detection**: HH:MM - [Description]
- **Response**: HH:MM - [Description]
- **Investigation**: HH:MM - [Description]
- **Resolution**: HH:MM - [Description]
- **Recovery**: HH:MM - [Description]

## Root Cause
[Detailed explanation of root cause]

## Impact Assessment
- **Users Affected**: [Number/Percentage]
- **Services Affected**: [List]
- **Data Impact**: [Description]
- **Financial Impact**: [Estimate]

## Actions Taken
1. [Action 1]
2. [Action 2]
3. [Action 3]

## Lessons Learned
- **What Went Well**: [List]
- **What Could Be Improved**: [List]
- **Process Improvements**: [List]

## Action Items
- [ ] [Action Item 1] - [Owner] - [Due Date]
- [ ] [Action Item 2] - [Owner] - [Due Date]
- [ ] [Action Item 3] - [Owner] - [Due Date]
```

#### Metrics Collection
```bash
# Collect incident metrics
./scripts/collect-incident-metrics.sh INCIDENT_ID

# Generate incident report
./scripts/generate-incident-report.sh INCIDENT_ID
```

### Improvement Planning

#### Process Improvements
1. **Detection**: Improve alerting and monitoring
2. **Response**: Streamline response procedures
3. **Communication**: Enhance communication protocols
4. **Recovery**: Optimize recovery procedures
5. **Prevention**: Implement preventive measures

#### Technical Improvements
1. **Monitoring**: Add missing metrics and alerts
2. **Automation**: Automate recovery procedures
3. **Testing**: Improve testing and validation
4. **Documentation**: Update runbooks and procedures
5. **Training**: Conduct team training sessions

## Common Incident Scenarios

### Database Incidents

#### Database Connection Issues
```bash
# Symptoms
- API errors: "database connection failed"
- High error rates in logs
- Application timeouts

# Investigation
docker compose exec postgres pg_isready -U postgres
docker compose exec postgres psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"

# Resolution
docker compose restart postgres
# Check connection pool settings
# Verify database credentials
```

#### Database Performance Issues
```bash
# Symptoms
- Slow API response times
- High database CPU usage
- Query timeouts

# Investigation
docker compose exec postgres psql -U postgres -c "SELECT query, mean_time, calls FROM pg_stat_statements WHERE mean_time > 1000 ORDER BY mean_time DESC LIMIT 10;"

# Resolution
# Optimize slow queries
# Add missing indexes
# Adjust connection pool settings
```

### Application Incidents

#### API Gateway Issues
```bash
# Symptoms
- 502/503 errors
- High response times
- Service unavailable

# Investigation
docker compose logs api-gateway --tail=100
curl -s http://localhost:3001/health | jq .

# Resolution
docker compose restart api-gateway
# Check upstream services
# Verify load balancer configuration
```

#### WebSocket Issues
```bash
# Symptoms
- Connection failures
- Message delivery issues
- High disconnection rates

# Investigation
docker compose logs websocket --tail=100
curl -s http://localhost:3002/health | jq .

# Resolution
docker compose restart websocket
# Check WebSocket configuration
# Verify message queue status
```

### Infrastructure Incidents

#### VPS/Server Issues
```bash
# Symptoms
- Complete service outage
- SSH connection failures
- High system load

# Investigation
# Check VPS provider status
# Verify network connectivity
# Check system resources

# Resolution
# Restart VPS if possible
# Contact VPS provider
# Implement failover procedures
```

#### Docker Issues
```bash
# Symptoms
- Container failures
- Resource exhaustion
- Service unavailability

# Investigation
docker compose ps
docker stats --no-stream
docker system df

# Resolution
docker compose restart
docker system prune -f
# Check resource limits
# Optimize container configuration
```

### Security Incidents

#### DDoS Attacks
```bash
# Symptoms
- High traffic volume
- Resource exhaustion
- Service degradation

# Investigation
# Check traffic patterns
# Monitor resource usage
# Analyze access logs

# Resolution
# Implement rate limiting
# Block malicious IPs
# Scale resources if needed
```

#### Authentication Issues
```bash
# Symptoms
- Login failures
- Session management issues
- Unauthorized access

# Investigation
docker compose logs auth-service --tail=100
# Check authentication logs
# Verify JWT configuration

# Resolution
# Restart auth service
# Check JWT secrets
# Verify database connectivity
```

## Escalation Procedures

### Escalation Matrix

#### P0 Incidents
- **Immediate**: Incident Commander (Phone)
- **15 minutes**: Technical Lead (Phone)
- **30 minutes**: CTO (Phone)
- **1 hour**: CEO (Phone)

#### P1 Incidents
- **Immediate**: Incident Commander (Slack)
- **30 minutes**: Technical Lead (Phone)
- **1 hour**: CTO (Slack)
- **2 hours**: CEO (Email)

#### P2 Incidents
- **Immediate**: On-call Engineer (Slack)
- **1 hour**: Team Lead (Slack)
- **4 hours**: Technical Lead (Email)

#### P3 Incidents
- **Immediate**: On-call Engineer (Ticket)
- **4 hours**: Team Lead (Ticket)
- **24 hours**: Technical Lead (Email)

### Escalation Triggers

#### Automatic Escalation
- **No response within SLA**: Automatic escalation
- **Multiple failed attempts**: Escalate to next level
- **Critical system failure**: Immediate escalation
- **Security breach**: Immediate escalation

#### Manual Escalation
- **Complex technical issues**: Escalate to technical lead
- **Business impact**: Escalate to management
- **External dependencies**: Escalate to vendor support
- **Resource constraints**: Escalate to management

## Tools and Resources

### Monitoring Tools

#### Prometheus
- **URL**: http://localhost:9090
- **Purpose**: Metrics collection and alerting
- **Access**: Operations team, Technical team

#### Grafana
- **URL**: http://localhost:3000
- **Purpose**: Metrics visualization and dashboards
- **Access**: Operations team, Technical team

#### Alertmanager
- **URL**: http://localhost:9093
- **Purpose**: Alert management and routing
- **Access**: Operations team, Technical team

### Communication Tools

#### Slack
- **Channels**: #incidents, #alerts, #operations
- **Purpose**: Internal communication
- **Access**: All team members

#### Status Page
- **URL**: https://status.chitlaq.com
- **Purpose**: External communication
- **Access**: Communications team

#### Phone/Email
- **Purpose**: Emergency communication
- **Access**: Incident response team

### Documentation Tools

#### Runbooks
- **Location**: /opt/chitlaq/runbooks/
- **Purpose**: Operational procedures
- **Access**: Operations team

#### Incident Reports
- **Location**: /opt/chitlaq/incidents/
- **Purpose**: Incident documentation
- **Access**: All team members

#### Knowledge Base
- **Location**: /opt/chitlaq/docs/
- **Purpose**: Technical documentation
- **Access**: All team members

### Recovery Tools

#### Backup Scripts
- **Location**: /opt/chitlaq/backup-scripts/
- **Purpose**: Data backup and recovery
- **Access**: Operations team

#### Deployment Scripts
- **Location**: /opt/chitlaq/scripts/
- **Purpose**: Service deployment and management
- **Access**: Operations team, Technical team

#### Health Check Scripts
- **Location**: /opt/chitlaq/scripts/
- **Purpose**: System health monitoring
- **Access**: Operations team

## Conclusion

This incident response runbook provides comprehensive procedures for handling various types of incidents in the ChitLaq M1 MVP infrastructure. Following these procedures ensures effective incident response, minimizes impact on users, and maintains system reliability.

Key incident response principles:
1. **Speed**: Rapid detection and response
2. **Communication**: Clear and timely updates
3. **Documentation**: Thorough incident documentation
4. **Learning**: Continuous improvement from incidents
5. **Prevention**: Proactive measures to prevent future incidents

Regular review and updates of this runbook ensure it remains current and effective for incident response team members.

---

**Document Version**: 1.0  
**Last Updated**: 2024  
**Next Review**: Monthly  
**Maintained by**: ChitLaq Operations Team
